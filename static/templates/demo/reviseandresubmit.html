<div class="bg-network">
  <div layout="row" layout-align="center">
    <!-- light section logo-overlay -->
    <!-- logo-container -->
    <div class="logo-overlay" flex-xs="100" flex="60">
      <div class="logo-container">
        <h2 class="logo-text">Revise and resubmit: why one crowd worker beats one million</h2>
        <p class="logo-subtitle">Reject considered harmful</p>
        <p class="logo-subtitle"><a href="http://hci.stanford.edu/msb">Michael Bernstein</a>, Aug 8 2017</p>
      </div>
    </div>
  </div>
</div>


<div class="light section textblock" id="intro">
  <div layout="row" layout-align="center" layout-margin="">
    <div flex-xs="90" flex="60" class="free-height">
      <p>Amazon Mechanical Turk pushes a terribly binary model: accept, or reject. Either your work is perfect, or <i>:slicing-motion:</i> you're cut. But <a href="http://hci.stanford.edu/publications/2017/flashorgs/flash-orgs-chi-2017.pdf">almost nothing can be rendered perfectly clearly into a task template</a>, and so <a href="https://idl.cs.washington.edu/files/2016-CrowdParting-CSCW.pdf">lots of work has "errors" stemming from multiple possible interpretations</a>. This reject-a-thon is a recipe for frustration, thrashing, and low-quality results.</p>

      <p>With <a href="http://www.daemo.org">Daemo</a>, we're experimenting with an alternative model where the default is to Revise rather than Reject. A revision routes the submission back to the worker, with feedback text to help guide the worker's edits. The worker can then make any changes and resubmit the work for approval. If things fail to converge, rejection is still possible, but we expect that this is a last resort rather than a default. It's still early days on our research platform, but digging across about 10,000 tasks, about 1.5% got returned.</p>

      <p>This is not only more worker-friendly; I suspect it also leads to higher-quality results. As a case study, I recently used Daemo to produce a predicate-logic format version of several undergraduate program sheets &mdash; sets of requirements to complete a major &mdash; at Stanford. These are notoriously complicated beasts, including beautiful instructions such as <i>"MATH 19 Calculus (see note 1), MATH 20, MATH 21. Note (1) Math 41, 42 may be taken instead of Math 19, 20, 21 as long as at least 26 math units are taken."</i> The resulting Daemo task is a total mouthful with lots of edge cases:</p>

      <div class="project-preview">
        <div project-preview="88"></div>
      </div>

      <p>When I posted <a href="https://arxiv.org/abs/1707.05645">a prototype</a> for this project on Daemo, nobody took it. I asked around on the forum, and the general consensus was that this task just seemed too complicated and tricky. One worker, Scott, offered to give it a shot anyway.</p>

      <p>Scott and I then iterated over a series of revisions, where we worked together to make sense of how to structure the program sheets with each successive revision. Scott would submit his thoughts, I'd compare to the program sheet and suggest a revision with clarifications on any misinterpretations, and I'd update the task description. When we'd aligned, I sent Scott a bonus for the extra time he spent coming up to speed on the task, and then he tackled the rest of the program sheets.</p>

      <p>This task could not have been readily achieved through aggressive quality filtering: nobody was even willing to try it initially! It was only through the revision process where Scott and I trained each other. He's now a localized expert on Stanford program sheets &mdash; maybe the only one, ever &mdash; and I have someone who is fast and accurate.</p>

      <p>I asked Scott, the worker, to share his feelings about the experience — both the good and the bad. He said:</p>
        <blockquote><i>"I wouldn’t have taken on a task like this on MTurk as it would have ended up being a waste of my time and effort due to rejections. On Daemo I was able to submit the task after having completed it knowing that if there were errors or misunderstandings I would have the opportunity to fix them. That I could do revisions really made all the difference. The only potential downside I can think of (which doesn’t apply to the task I did) is that if a task isn’t explained well enough at the beginning it could take far longer than anticipated to complete it. Being able to send the task back to the person doing it shouldn’t be relied on too heavily in this regard as it should still be relatively adequately explained from the get-go."</i></blockquote>
      <p>I agree about the potential risk for abuse. One route would be to let the worker contest the revision request. But a more direct way we seek to counteract such abuse on Daemo is the prototype task — a story for another time — which allows workers to prevent a task from launching in the first place if it's underpaying or too confusing.
      </p>

      <h1 class="heading">Some takeaways</h1>
      <ul>
        <li>Revision gives workers more space to try complex tasks and iterate with you to achieve high quality results.</li>
        <li>Once a worker is confident that the revision process has taught them the ins and outs of the task to a level that you approve of, the tasks can get done very quickly as that worker plows through them.</li>
        <li>University program sheets are the spawn of Hades.</li>
      </ul>

      <p>Feel free to ping me at <a href="mailto:msb@cs.stanford.edu">msb@cs.stanford.edu</a> or <a
        href="http://twitter.com/msbernst">@msbernst</a>. If you'd like to get access to Daemo to try it out while we finish developing it as a research crowdsourcing platform, let me know. Thanks to Daemo worker Scott for their contributions to this project!</p>
    </div>
  </div>
</div>
